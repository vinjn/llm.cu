# Setup

- Download gpt2 weights from  https://huggingface.co/openai-community/gpt2/resolve/main/model.safetensors

# TODO

- [ ] Parse all the layers
- [ ] Tokenizer
- [ ] matmul in C and CUDA
- [ ] MLP in C and CUDA
- [ ] attention layer in C and CUDA


# Reference

- https://www.omrimallis.com/posts/understanding-how-llm-inference-works-with-llama-cpp/
- https://jalammar.github.io/illustrated-transformer/
- https://dugas.ch/artificial_curiosity/GPT_architecture.html

